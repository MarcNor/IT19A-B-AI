{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch', '.']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"Mary, don't slap the green witch.\"\n",
    "print([str(token) for token in nlp(text.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['if', 'we', 'don', '’', 't', 'stand', 'up', 'for', 'democracy', ',', 'we', '’', 'll', 'see', 'this', 'type', 'of', 'gerrymandering', 'happen', 'in', 'even', 'more', 'states', '.', 'it', '’', 's', 'why', '@demredistrict', 'is', 'fighting', 'for', '#fairmaps', '.', 'and', 'it', '’', 's', 'why', 'the', 'senate', 'must', 'pass', 'the', 'freedom', 'to', 'vote', 'act', 'to', 'protect', 'our', 'communities', 'from', 'partisan', 'gerrymandering', '.']\n"
     ]
    }
   ],
   "source": [
    "tweet = u\"If we don’t stand up for democracy, we’ll see this type of gerrymandering happen in even more states. It’s why @DemRedistrict is fighting for #FairMaps. And it’s why the Senate must pass the Freedom to Vote Act to protect our communities from partisan gerrymandering.\"\n",
    "tokenizer = TweetTokenizer()\n",
    "print(tokenizer.tokenize(tweet.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(text, n):\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mary', ',', 'do'], [',', 'do', \"n't\"], ['do', \"n't\", 'slap'], [\"n't\", 'slap', 'the'], ['slap', 'the', 'green'], ['the', 'green', 'witch'], ['green', 'witch', '.']]\n"
     ]
    }
   ],
   "source": [
    "text = ['mary', ',', 'do', \"n't\", 'slap', 'the', 'green', 'witch', '.']\n",
    "print(n_grams(text, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n"
     ]
    }
   ],
   "source": [
    "example_words = [\"better\"]\n",
    "\n",
    "for w in example_words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/marc/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well\n"
     ]
    }
   ],
   "source": [
    "sp = spacy.load('en_core_web_sm')\n",
    "token = sp(u'better')\n",
    "for word in token:\n",
    "    print(word.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Charles Spencer Chaplin was born on 16 April 1889 to Hannah Chaplin (born Hannah Harriet Pedlingham Hill) and Charles Chaplin Sr .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Charles PROPN Xxxxx True False\n",
      "Spencer Spencer PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "was be AUX xxx True True\n",
      "born bear VERB xxxx True False\n",
      "on on ADP xx True True\n",
      "16 16 NUM dd False False\n",
      "April April PROPN Xxxxx True False\n",
      "1889 1889 NUM dddd False False\n",
      "to to ADP xx True True\n",
      "Hannah Hannah PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "( ( PUNCT ( False False\n",
      "born bear VERB xxxx True False\n",
      "Hannah Hannah PROPN Xxxxx True False\n",
      "Harriet Harriet PROPN Xxxxx True False\n",
      "Pedlingham Pedlingham PROPN Xxxxx True False\n",
      "Hill Hill PROPN Xxxx True False\n",
      ") ) PUNCT ) False False\n",
      "and and CCONJ xxx True True\n",
      "Charles Charles PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "Sr Sr PROPN Xx True False\n",
      ". . PUNCT . False False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles compound Chaplin PROPN []\n",
      "Spencer compound Chaplin PROPN []\n",
      "Chaplin nsubjpass born VERB [Charles, Spencer]\n",
      "was auxpass born VERB []\n",
      "born ROOT born VERB [Chaplin, was, on, to, .]\n",
      "on prep born VERB [April]\n",
      "16 nummod April PROPN []\n",
      "April pobj on ADP [16, 1889]\n",
      "1889 nummod April PROPN []\n",
      "to prep born VERB [Chaplin]\n",
      "Hannah compound Chaplin PROPN []\n",
      "Chaplin pobj to ADP [Hannah, (, born, ), and, Sr]\n",
      "( punct Chaplin PROPN []\n",
      "born acl Chaplin PROPN [Hill]\n",
      "Hannah compound Hill PROPN []\n",
      "Harriet compound Hill PROPN []\n",
      "Pedlingham compound Hill PROPN []\n",
      "Hill dobj born VERB [Hannah, Harriet, Pedlingham]\n",
      ") punct Chaplin PROPN []\n",
      "and cc Chaplin PROPN []\n",
      "Charles compound Sr PROPN []\n",
      "Chaplin compound Sr PROPN []\n",
      "Sr conj Chaplin PROPN [Charles, Chaplin]\n",
      ". punct born VERB []\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Mary slapped the green witch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary - NP\n",
      "the green witch - NP\n"
     ]
    }
   ],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(\"{} - {}\".format(chunk, chunk.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "test = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(binary=True)\n",
    "count_vect.fit(processed_docs)\n",
    "test = count_vect.transform([\"dog and dog are friends\"])\n",
    "print(\"Bow representation for 'dog and dog are friends':\", test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"]\n",
    "processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dog': 3,\n",
       " 'bites': 0,\n",
       " 'man': 12,\n",
       " 'dog bites': 4,\n",
       " 'bites man': 2,\n",
       " 'dog bites man': 5,\n",
       " 'man bites': 13,\n",
       " 'bites dog': 1,\n",
       " 'man bites dog': 14,\n",
       " 'eats': 8,\n",
       " 'meat': 17,\n",
       " 'dog eats': 6,\n",
       " 'eats meat': 10,\n",
       " 'dog eats meat': 7,\n",
       " 'food': 11,\n",
       " 'man eats': 15,\n",
       " 'eats food': 9,\n",
       " 'man eats food': 16}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,3))\n",
    "bow_rep = count_vect.fit_transform(processed_docs)\n",
    "count_vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW representation for 'dog bites man':  [[1 0 1 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0]]\n",
      "BoW representation for 'man bites dog:  [[1 1 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n",
    "print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bow representation for 'dog and dog are friends': [[0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "test = count_vect.transform([\"dog and dog are friends\"])\n",
    "\n",
    "print(\"Bow representation for 'dog and dog are friends':\", test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
